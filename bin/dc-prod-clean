#!/usr/bin/env python

import os
import copy
import pickle
import argparse
import logging


import torch
from torch.utils.data import DataLoader

import deepclean_prod as dc
from deepclean_prod import config

from gwpy.timeseries import TimeSeries

# Default tensor type
torch.set_default_tensor_type(torch.FloatTensor)


def parse_cmd():
    
    parser = argparse.ArgumentParser(
        prog=os.path.basename(__file__), usage='%(prog)s [options]')
    
    # Dataset arguments
    parser.add_argument('--clean-t0', help='GPS of the first sample', type=int)
    parser.add_argument('--clean-duration', help='Duration of frame', type=int)
    parser.add_argument('--chanslist', help='Path to channel list', type=str)
    parser.add_argument('--fs', help='Sampling frequency', 
                        default=config.DEFAULT_SAMPLE_RATE, type=float)

    # Timeseries arguments
    parser.add_argument('--clean-kernel', help='Length of each segment in seconds', 
                        default=config.DEFAULT_CLEAN_KERNEL, type=float)
    parser.add_argument('--clean-stride', help='Stride between segments in seconds', 
                        default=config.DEFAULT_CLEAN_STRIDE, type=float)
    parser.add_argument('--pad-mode', help='Padding mode', 
                        default=config.DEFAULT_PAD_MODE, type=str)
    
    # Post-processing arguments
    parser.add_argument('--window', help='Window to apply to overlap-add',
                        default=config.DEFAULT_WINDOW, type=str)

    # Input/output arguments    
    parser.add_argument('--train-dir', help='Path to training directory', 
                        default='.', type=str)
    parser.add_argument('--checkpoint', help='Path to model checkpoint', type=str)
    parser.add_argument('--ppr-file', help='Path to preprocessing setting', type=str)
    parser.add_argument('--out-dir', help='Path to output file',
                        default='.', type=str)
    parser.add_argument('--out-file', help='Path to output file', type=str)
    parser.add_argument('--out-channel', help='Name of output channel', type=str)
    parser.add_argument('--load-dataset', help='Load dataset',
                        default=False, type=dc.io.str2bool)
    parser.add_argument('--save-dataset', help='Save dataset', 
                        default=False, type=dc.io.str2bool)
    parser.add_argument('--log', help='Log file', type=str)

    # cuda arguments
    parser.add_argument('--device', help='Device to use', 
                         default=config.DEFAULT_DEVICE, type=str)
    
    params = parser.parse_args()

    return params

params =  parse_cmd()

# Create output directory
if params.out_dir is not None:
    os.makedirs(params.out_dir, exist_ok=True)
if params.log is not None:
    params.log = os.path.join(params.out_dir, params.log)
logging.basicConfig(filename=params.log, filemode='a', 
                    format='%(asctime)s - %(message)s', level=logging.DEBUG)
logging.info('Create training directory: {}'.format(params.out_dir))

# Use GPU if available
device = dc.nn.utils.get_device(params.device)

# Get time-series data from NDS server or local
data = dc.timeseries.TimeSeriesSegmentDataset(
    params.clean_kernel, params.clean_stride, params.pad_mode)

download_data = True
original_dataset_filename = os.path.join(params.out_dir, 'original-{}-{}.h5'.format(
    params.clean_t0, params.clean_duration))

# Decide whether or not to download data
if (params.clean_t0 is None) and (params.clean_duration is None):
    logging.info('GPS Time not given. Fetching from file: {}'.format(
        original_dataset_filename))
    download_data = False
    if not os.path.exists(original_dataset_filename):
        raise FileNotFoundError('Cannot find dataset file. File or GPS time must exist')
elif params.load_dataset:
    logging.info('Attempting to fetch from dataset file {}'.format(
        original_dataset_filename))
    download_data = False
    if not os.path.exists(original_dataset_filename):
        logging.info('Cannot find dataset file. Using GPS Time')
        download_data = True
    
# Download data if needed
if download_data:
    logging.info('Fetching raw data from {} .. {}'.format(
        params.clean_t0, params.clean_t0 + params.clean_duration))
    data.fetch(params.chanslist, params.clean_t0, params.clean_duration, params.fs)
    if params.save_dataset:
        data.write(original_dataset_filename)
else:
    logging.info('Fetching data from file {}'.format(original_dataset_filename))
    data.read(original_dataset_filename, params.chanslist)


# Preprocess data
logging.info('Preprocessing')
# fetch preprocess setting
if params.ppr_file is None:
    params.ppr_file = os.path.join(params.train_dir, 'ppr.bin')
with open(params.ppr_file, 'rb') as f:
    ppr = pickle.load(f)
    mean = ppr['mean']
    std = ppr['std']
    filt_fl = ppr['filt_fl']
    filt_fh = ppr['filt_fh']
    filt_order = ppr['filt_order']
# bandpass filter and normalization
preprocessed = data.bandpass(filt_fl, filt_fh, filt_order, 'target')
preprocessed = preprocessed.normalize(mean, std)
        
# Load preprocessed data into dataloader
data_loader = DataLoader(
    preprocessed, batch_size=config.DEFAULT_BATCH_SIZE,
    num_workers=config.DEFAULT_NUM_WORKERS, shuffle=False)

# Creating model and load from checkpoint
model = dc.nn.net.DeepClean(preprocessed.n_channels - 1)
model = model.to(device)  # transfer model to GPU if available
logging.info(model)

if params.checkpoint is None:
    params.checkpoint = dc.nn.utils.get_last_checkpoint(
        os.path.join(params.train_dir, 'models'))
logging.info('Loading model from checkpoint: {}'.format(params.checkpoint))
model.load_state_dict(torch.load(params.checkpoint, map_location=device))

# Start data cleaning
logging.info('Cleaning')
target = data.get_target()  # get raw data

# predict noise from auxilliary channels
pred_batches = dc.nn.utils.evaluate(data_loader, model, device=device)

# post-processing the prediction
# overlap-add to join together prediction
noverlap = int((preprocessed.kernel - preprocessed.stride) * preprocessed.fs)
pred = dc.signal.overlap_add(pred_batches, noverlap, params.window)
# convert back to unit of target
pred *= std[preprocessed.target_idx].ravel()
pred += mean[preprocessed.target_idx].ravel()
# apply bandpass filter
pred = dc.signal.filter_add(pred, preprocessed.fs, filt_fl, filt_fh, filt_order)
# because of data padding, we apply a cut to the output
pred = pred[:target.size]

# subtract noise prediction from raw data
clean = target - pred 

# Save clean data
if params.out_file is None:
    params.out_file = os.path.join(params.out_dir, 'clean-{}-{}.h5'.format(
        params.clean_t0, params.clean_duration))
else:
    params.out_file = os.path.join(params.out_dir, params.out_file)
    
logging.info('Writing output to {}'.format(params.out_file))
series = TimeSeries(clean, t0=data.t0, sample_rate=data.fs, 
                    channel=params.out_channel, name=params.out_channel)
series.write(params.out_file)
